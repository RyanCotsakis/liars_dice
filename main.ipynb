{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "869ee266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b89f12d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2' '3' '4' '5' '6' 'L' '22' '33' '44' '55' '66' 'LL' 'D']\n",
      "Connecting deepest nodes\n"
     ]
    }
   ],
   "source": [
    "N_DICE = (1, 1) # tuple (number of dice P1, number of dice P2)\n",
    "\n",
    "class Die():\n",
    "    faces = [\"L\", \"2\", \"3\", \"4\", \"5\", \"6\"] # Llamas first\n",
    "\n",
    "    def __init__(self):\n",
    "        self.roll()\n",
    "    \n",
    "    # roll the die n times and return the result as a list\n",
    "    def roll(self, n=1):\n",
    "        result = np.random.choice(Die.faces, n)\n",
    "        self.result = result[-1]\n",
    "        return result\n",
    "        \n",
    "\n",
    "class Player():\n",
    "    action_space = np.concat(\n",
    "        [[face * i for face in Die.faces] for i in range(1,np.sum(N_DICE) + 1)] +\n",
    "        [[\"L\" * i for i in range(np.sum(N_DICE) + 1, 2 * np.sum(N_DICE) + 1)]] +\n",
    "        # [[\"D\", \"C\"]] # doubt or call\n",
    "        [[\"D\"]] # just doubt\n",
    "    )\n",
    "    marked_for_removal = []\n",
    "    for i, el in enumerate(action_space):\n",
    "        if el[0] == \"L\":\n",
    "            if len(el) % 2 == 1:\n",
    "                marked_for_removal.append(i)\n",
    "            else:\n",
    "                action_space[i] = el[:len(el)//2]\n",
    "    action_space = np.delete(action_space, marked_for_removal)\n",
    "    print(action_space)\n",
    "\n",
    "    def __init__(self, player_id):\n",
    "        self.player_id = player_id\n",
    "        self.die = Die()\n",
    "        self.private = self.die.roll(N_DICE[player_id])\n",
    "\n",
    "\n",
    "class Node():\n",
    "    MAX_DEPTH = 3\n",
    "    n_nodes = 0\n",
    "    leaves = []\n",
    "    deep_nodes = [] # these are not leaves\n",
    "\n",
    "    def roll2index(roll):\n",
    "        index = 0\n",
    "        for i, face in enumerate(roll):\n",
    "            d = Die.faces.index(face)\n",
    "            index += (len(Die.faces) ** i) * d\n",
    "        return index\n",
    "\n",
    "    def index2roll(index, n_dice):\n",
    "        roll = []\n",
    "        for i in range(n_dice):\n",
    "            roll.append(Die.faces[index % len(Die.faces)])\n",
    "            index //= len(Die.faces)\n",
    "        return roll\n",
    "\n",
    "    def __init__(self, parent, last_action=None):\n",
    "        self.parent = parent\n",
    "        if parent is None:\n",
    "            self.depth = 0\n",
    "            self.player = 0 # start with player zero\n",
    "            self.probability = torch.ones([len(Die.faces) ** n for n in N_DICE]) # certain to hit this node\n",
    "            Node.player_logits = [[], []]\n",
    "            Node.n_nodes = 0\n",
    "        else:\n",
    "            self.depth = self.parent.depth + 1\n",
    "            self.player = (self.parent.player + 1) % 2\n",
    "            self.probability = torch.zeros([len(Die.faces) ** n for n in N_DICE])\n",
    "            if self.player:\n",
    "                self.probability = self.probability.t()\n",
    "        Node.n_nodes += 1\n",
    "        self.last_action = last_action\n",
    "        self.is_leaf = last_action == \"D\" or last_action == \"C\"\n",
    "        self.children = {} # empty dictionary. Keys are actions and values are Nodes.\n",
    "        if self.is_leaf:\n",
    "            self.logits = None\n",
    "            Node.leaves.append(self)\n",
    "            self.compute_winner_matrix()\n",
    "        else:\n",
    "            self.winner = None\n",
    "            if last_action is None:\n",
    "                starting_index = 0\n",
    "            else:\n",
    "                starting_index = np.where(Player.action_space == last_action)[0][0] + 1\n",
    "            possible_actions = Player.action_space[starting_index:]\n",
    "            if last_action is None:\n",
    "                possible_actions = np.delete(possible_actions,\n",
    "                                             np.where([x[0] in (\"L\", \"D\", \"C\") for x in possible_actions])[0])\n",
    "            n_actions = len(possible_actions)\n",
    "            n_private = len(Die.faces) ** N_DICE[self.player]\n",
    "\n",
    "            self.logits = torch.ones(n_private, n_actions, requires_grad=True)\n",
    "            Node.player_logits[self.player].append(self.logits)\n",
    "            \n",
    "            if self.depth < Node.MAX_DEPTH:\n",
    "                for action in possible_actions:\n",
    "                    self.children[action] = Node(parent=self, last_action=action)\n",
    "            else:\n",
    "                previous_node = self\n",
    "                for i in range(Node.MAX_DEPTH - 3):\n",
    "                    previous_node = previous_node.parent\n",
    "                would_start_with = previous_node.last_action\n",
    "                if (would_start_with[0] == \"L\") or ((Node.MAX_DEPTH - self.player) % 2):\n",
    "                    for action in possible_actions:\n",
    "                        self.children[action] = Node(parent=self, last_action=action)\n",
    "                else:\n",
    "                    Node.deep_nodes.append(self)\n",
    "\n",
    "        if parent is None: # start connecting the deep nodes\n",
    "            print(\"Connecting deepest nodes\")\n",
    "            for node in Node.deep_nodes:\n",
    "                previous_actions = []\n",
    "                previous_node = node\n",
    "                for i in range(Node.MAX_DEPTH - 2): # minus 2 to match the players\n",
    "                    previous_actions.append(previous_node.last_action)\n",
    "                    previous_node = previous_node.parent\n",
    "                # Now find the Node that is similar to node but one depth less.\n",
    "                next_node = self\n",
    "                for i in range(Node.MAX_DEPTH - 2):\n",
    "                    next_node = next_node.children[previous_actions.pop()]\n",
    "                node.children = next_node.children\n",
    "    \n",
    "    def propagate_probability(self, probability):\n",
    "        # the probability is a matrix with rows possible private info of self.player and\n",
    "        # columns possible private info of the opponent. Each entry corresponds to the\n",
    "        # conditional probability of arriving at that node given the private information.\n",
    "        softmaxed = F.softmax(self.logits, dim=-1)\n",
    "        for i, child in enumerate(self.children.values()): # correct order verified.\n",
    "            give_prob = softmaxed[:, i].unsqueeze(1)\n",
    "            new_probability = (give_prob * probability).t()\n",
    "            child.probability += new_probability\n",
    "            if not child.is_leaf:\n",
    "                child.propagate_probability(new_probability)\n",
    "    \n",
    "    def reset_probability(self):\n",
    "        if self.parent is not None:\n",
    "            self.probability = torch.zeros([len(Die.faces) ** n for n in N_DICE])\n",
    "            if self.player:\n",
    "                self.probability = self.probability.t()\n",
    "        if not self.is_leaf:\n",
    "            for child in self.children.values():\n",
    "                if child.depth > self.depth:\n",
    "                    child.reset_probability()\n",
    "\n",
    "    def who_wins(self, my_roll, opponent_roll):\n",
    "        assert self.is_leaf\n",
    "        all_dice = my_roll + opponent_roll\n",
    "        claim = self.parent.last_action # This is my claim\n",
    "        response = self.last_action # call C or doubt D. My opponent said this.\n",
    "        face = claim[0]\n",
    "        quantity = len(claim)\n",
    "        true_count = all_dice.count(face)\n",
    "        if face != \"L\":\n",
    "            true_count += all_dice.count(\"L\")\n",
    "        if response == \"C\":\n",
    "            if quantity == true_count:\n",
    "                return 1 - self.player\n",
    "            return self.player\n",
    "        if response == \"D\":\n",
    "            if true_count >= quantity:\n",
    "                return self.player\n",
    "            return 1 - self.player\n",
    "\n",
    "    def compute_winner_matrix(self):\n",
    "        assert self.is_leaf\n",
    "        self.winner = torch.zeros([len(Die.faces) ** n for n in N_DICE]) # zeros or ones does not matter\n",
    "        if self.player == 1:\n",
    "            self.winner = self.winner.t()\n",
    "        shape = self.winner.shape\n",
    "        for i in range(shape[0]): # my roll index\n",
    "            for j in range(shape[1]): # opponent roll index\n",
    "                my_roll = Node.index2roll(i, N_DICE[self.player])\n",
    "                opponent_roll = Node.index2roll(j, N_DICE[1 - self.player])\n",
    "                winner = self.who_wins(my_roll, opponent_roll)\n",
    "                self.winner[i, j] = winner\n",
    "\n",
    "\n",
    "PLAYERS = [Player(i) for i in range(len(N_DICE))]\n",
    "grandfather_node = Node(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285f9f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter    0   Prob(P1 wins) ≈  0.4877\n",
      "iter   10   Prob(P1 wins) ≈  0.4899\n",
      "iter   20   Prob(P1 wins) ≈  0.4915\n",
      "iter   30   Prob(P1 wins) ≈  0.4925\n",
      "iter   40   Prob(P1 wins) ≈  0.4928\n",
      "iter   50   Prob(P1 wins) ≈  0.4921\n",
      "iter   60   Prob(P1 wins) ≈  0.4901\n",
      "iter   70   Prob(P1 wins) ≈  0.4872\n",
      "iter   80   Prob(P1 wins) ≈  0.4846\n",
      "iter   90   Prob(P1 wins) ≈  0.4839\n",
      "iter  100   Prob(P1 wins) ≈  0.4859\n",
      "iter  110   Prob(P1 wins) ≈  0.4896\n",
      "iter  120   Prob(P1 wins) ≈  0.4933\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m it % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     27\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33miter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mit\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m4d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m   Prob(P1 wins) ≈ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprob_1_wins.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m .4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[43mcalculate_equilibrium\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mcalculate_equilibrium\u001b[39m\u001b[34m(lr)\u001b[39m\n\u001b[32m     16\u001b[39m opt_p0.zero_grad()\n\u001b[32m     17\u001b[39m prob_1_wins = get_prob_1_wins()\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mprob_1_wins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m opt_p0.step()\n\u001b[32m     21\u001b[39m opt_p1.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rcotsaki\\Code\\liars_dice\\.venv\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rcotsaki\\Code\\liars_dice\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rcotsaki\\Code\\liars_dice\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def get_prob_1_wins():\n",
    "    grandfather_node.reset_probability()\n",
    "    grandfather_node.propagate_probability(grandfather_node.probability)\n",
    "    stacked = torch.stack([(leaf.probability * leaf.winner).t() if leaf.player == 1\n",
    "                    else (leaf.probability * leaf.winner) for leaf in Node.leaves],\n",
    "                    dim=0)\n",
    "    prob_1_wins = torch.sum(stacked) / (len(Die.faces) ** np.sum(N_DICE))\n",
    "    return prob_1_wins\n",
    "\n",
    "\n",
    "def calculate_equilibrium(lr = 0.01):\n",
    "    opt_p0 = torch.optim.Adam(Node.player_logits[0], lr=lr)\n",
    "    opt_p1 = torch.optim.Adam(Node.player_logits[1], lr=lr)\n",
    "\n",
    "    for it in range(2000):\n",
    "        opt_p0.zero_grad()\n",
    "        prob_1_wins = get_prob_1_wins()\n",
    "        prob_1_wins.backward()\n",
    "        opt_p0.step()\n",
    "\n",
    "        opt_p1.zero_grad()\n",
    "        prob_1_wins = get_prob_1_wins()\n",
    "        (-prob_1_wins).backward()\n",
    "        opt_p1.step()\n",
    "\n",
    "        if it % 10 == 0:\n",
    "            print(f\"iter {it:4d}   Prob(P1 wins) ≈ {prob_1_wins.item(): .4f}\")\n",
    "\n",
    "calculate_equilibrium()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e23da996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000],\n",
      "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000],\n",
      "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000],\n",
      "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000],\n",
      "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000],\n",
      "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000]], grad_fn=<SoftmaxBackward0>)\n",
      "726\n",
      "240\n",
      "486\n",
      "[121, 365]\n",
      "tensor([[1., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(F.softmax(grandfather_node.logits, dim=-1))\n",
    "print(Node.n_nodes)\n",
    "print(len(Node.leaves))\n",
    "print(Node.n_nodes - len(Node.leaves))\n",
    "print([len(x) for x in Node.player_logits])\n",
    "print(grandfather_node.children[\"2\"].children[\"33\"].children[\"D\"].winner)\n",
    "grandfather_node.reset_probability()\n",
    "grandfather_node.propagate_probability(grandfather_node.probability)\n",
    "stacked = torch.stack([leaf.probability.t() if leaf.player == 1\n",
    "                    else leaf.probability for leaf in Node.leaves],\n",
    "                    dim=0)\n",
    "torch.sum(stacked, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c4491",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Node.index2roll(Node.roll2index([\"L\", \"5\", \"4\"]), 3))\n",
    "print(Node.MAX_DEPTH)\n",
    "print(grandfather_node.children[\"66\"])\n",
    "print(grandfather_node.children[\"55\"].children[\"66\"])\n",
    "print(grandfather_node.children[\"44\"].children[\"55\"].children[\"66\"])\n",
    "print(grandfather_node.children[\"33\"].children[\"44\"].children[\"55\"].children[\"66\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244ec505",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(4, 1)\n",
    "b = torch.randn(4, 5)\n",
    "a = a.expand((4,5))\n",
    "print(a)\n",
    "Die.faces.index(\"3\")\n",
    "\n",
    "a = (1,2)\n",
    "a[1] = 11\n",
    "a\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fd2094",
   "metadata": {},
   "source": [
    "### Insights from Von Neuman's work and ChatGPT\n",
    "\n",
    "- At each step, for each player, alternating between them, I should optimize for the probability distribution that maximizes the active player's overall expected probability of winning.\n",
    "- This can be done by computing the expected winning probability of the *entire tree*, and maximizing the probability of the winning leaves by only changing the player's own nodes.\n",
    "\n",
    "Something like this:\n",
    "```python\n",
    "# Optimizers for max (P1) and min (P2)\n",
    "opt_p1 = torch.optim.Adam([p1_logits], lr=1e-1)\n",
    "opt_p2 = torch.optim.Adam([p2_logits], lr=1e-1)\n",
    "\n",
    "for it in range(5000):\n",
    "    # — P1 update (ascent) —\n",
    "    opt_p1.zero_grad()\n",
    "    ev = expected_payoff()\n",
    "    (-ev).backward(retain_graph=True)   # gradient of –E wrt p1_logits\n",
    "    opt_p1.step()\n",
    "\n",
    "    # — P2 update (descent) —\n",
    "    opt_p2.zero_grad()\n",
    "    ev = expected_payoff()\n",
    "    (ev).backward()                     # gradient of  E wrt p2_logits\n",
    "    opt_p2.step()\n",
    "\n",
    "    if it % 500 == 0:\n",
    "        print(f\"iter {it:4d}   EV ≈ {ev.item(): .4f}\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
