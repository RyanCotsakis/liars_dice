{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869ee266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89f12d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DICE = (1, 1) # tuple (number of dice P1, number of dice P2)\n",
    "\n",
    "class Die():\n",
    "    faces = [\"L\", \"2\", \"3\", \"4\", \"5\", \"6\"] # Llamas first\n",
    "\n",
    "    def __init__(self):\n",
    "        self.roll()\n",
    "    \n",
    "    # roll the die n times and return the result as a list\n",
    "    def roll(self, n=1):\n",
    "        result = np.random.choice(Die.faces, n)\n",
    "        self.result = result[-1]\n",
    "        return result\n",
    "\n",
    "\n",
    "class Player():\n",
    "    action_space = np.concat(\n",
    "        [[face * i for face in Die.faces] for i in range(1,np.sum(N_DICE) + 1)] +\n",
    "        [[\"L\" * i for i in range(np.sum(N_DICE) + 1, 2 * np.sum(N_DICE) + 1)]] +\n",
    "        # [[\"D\", \"C\"]] # doubt or call\n",
    "        [[\"D\"]] # just doubt\n",
    "    )\n",
    "    marked_for_removal = []\n",
    "    for i, el in enumerate(action_space):\n",
    "        if el[0] == \"L\":\n",
    "            if len(el) % 2 == 1:\n",
    "                marked_for_removal.append(i)\n",
    "            else:\n",
    "                action_space[i] = el[:len(el)//2]\n",
    "    action_space = np.delete(action_space, marked_for_removal)\n",
    "    print(action_space)\n",
    "\n",
    "    def faceoff(player_0, player_1):\n",
    "        player_0.opponent = player_1\n",
    "        player_1.opponent = player_0\n",
    "        if player_0.player_id == 0:\n",
    "            player_0.act(grandfather_node)\n",
    "        elif player_1.player_id == 0:\n",
    "            player_1.act(grandfather_node)\n",
    "\n",
    "    def __init__(self, player_id, human=False, physical_dice=False):\n",
    "        if not human:\n",
    "            physical_dice = False\n",
    "        self.human = human\n",
    "        self.opponent = None # corrected by Player.faceoff\n",
    "        self.player_id = player_id\n",
    "        self.die = Die()\n",
    "        if not physical_dice:\n",
    "            self.private = self.die.roll(N_DICE[player_id])\n",
    "            if human:\n",
    "                print(\"You rolled \" + str(self.private))\n",
    "        else:\n",
    "            self.private = None\n",
    "    \n",
    "    def act(self, node):\n",
    "        if self.human:\n",
    "            self.act_manual(node)\n",
    "        else:\n",
    "            self.act_automatic(node)\n",
    "\n",
    "    def act_manual(self, node):\n",
    "        assert self.human\n",
    "        if not self.opponent.human:\n",
    "            if node.last_action is not None and node.last_action not in (\"C\", \"D\"):\n",
    "                print(\"[Computer] I bet \" + node.last_action + \".\")\n",
    "            elif node.last_action is None:\n",
    "                print(\"[Computer] You start!\")\n",
    "            elif node.last_action == \"D\":\n",
    "                print(\"[Computer] I doubt that. I had \" + str(self.opponent.private))\n",
    "                return\n",
    "            else:\n",
    "                print(\"[Computer] I call. I had \" + str(self.opponent.private))\n",
    "                return\n",
    "        \n",
    "        possible_actions = [action for action in node.children]\n",
    "        print(\"Possible actions: \" + str(possible_actions))\n",
    "        action = input(\">> \")\n",
    "        assert action in possible_actions\n",
    "\n",
    "        if self.private is None:\n",
    "            possible_private = [str(Node.index2roll(i, N_DICE[self.player_id])) for i in range(len(Die.faces) ** N_DICE[self.player_id])]\n",
    "            move_matrix = F.softmax(node.logits, dim=-1)\n",
    "            df = pd.DataFrame(move_matrix.detach().numpy(), index=possible_private, columns=possible_actions)\n",
    "        else:\n",
    "            move_matrix = F.softmax(node.logits[Node.roll2index(self.private),:], dim=-1)\n",
    "            df = pd.DataFrame(move_matrix.detach().numpy(), index=str(self.private), columns=possible_actions)\n",
    "        print(\"The optimal moves were:\")\n",
    "        print(df)\n",
    "\n",
    "        next_node = node.children[action]\n",
    "        self.opponent.act(next_node)        \n",
    "        \n",
    "    def act_automatic(self, node):\n",
    "        assert not self.human\n",
    "        if node.is_leaf and self.opponent.human:\n",
    "            print(\"[Computer] I had \" + str(self.private))\n",
    "        move_matrix = F.softmax(node.logits[Node.roll2index(self.private),:], dim=-1).detach().numpy()\n",
    "        next_node = np.random.choice([child for child in node.children.values()], p=move_matrix)\n",
    "        self.opponent.act(next_node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482bda5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    MAX_DEPTH = 4\n",
    "    n_nodes = 0\n",
    "    leaves = []\n",
    "    deep_nodes = [] # these are not leaves\n",
    "\n",
    "    def roll2index(roll):\n",
    "        index = 0\n",
    "        for i, face in enumerate(roll):\n",
    "            d = Die.faces.index(face)\n",
    "            index += (len(Die.faces) ** i) * d\n",
    "        return index\n",
    "\n",
    "    def index2roll(index, n_dice):\n",
    "        roll = []\n",
    "        for i in range(n_dice):\n",
    "            roll.append(Die.faces[index % len(Die.faces)])\n",
    "            index //= len(Die.faces)\n",
    "        return roll\n",
    "\n",
    "    def __init__(self, parent, last_action=None):\n",
    "        self.parent = parent\n",
    "        if parent is None:\n",
    "            self.depth = 0\n",
    "            self.player = 0 # start with player zero\n",
    "            self.probability = torch.ones([len(Die.faces) ** n for n in N_DICE]) # certain to hit this node\n",
    "            Node.player_logits = [[], []]\n",
    "            Node.n_nodes = 0\n",
    "        else:\n",
    "            self.depth = self.parent.depth + 1\n",
    "            self.player = (self.parent.player + 1) % 2\n",
    "            self.probability = torch.zeros([len(Die.faces) ** n for n in N_DICE])\n",
    "            if self.player:\n",
    "                self.probability = self.probability.t()\n",
    "        Node.n_nodes += 1\n",
    "        self.last_action = last_action\n",
    "        self.is_leaf = last_action == \"D\" or last_action == \"C\"\n",
    "        self.children = {} # empty dictionary. Keys are actions and values are Nodes.\n",
    "        if self.is_leaf:\n",
    "            self.logits = None\n",
    "            Node.leaves.append(self)\n",
    "            self.compute_winner_matrix()\n",
    "        else:\n",
    "            self.winner = None\n",
    "            if last_action is None:\n",
    "                starting_index = 0\n",
    "            else:\n",
    "                starting_index = np.where(Player.action_space == last_action)[0][0] + 1\n",
    "            possible_actions = Player.action_space[starting_index:]\n",
    "            if last_action is None:\n",
    "                possible_actions = np.delete(possible_actions,\n",
    "                                             np.where([x[0] in (\"L\", \"D\", \"C\") for x in possible_actions])[0])\n",
    "            n_actions = len(possible_actions)\n",
    "            n_private = len(Die.faces) ** N_DICE[self.player]\n",
    "\n",
    "            self.logits = torch.ones(n_private, n_actions, requires_grad=True)\n",
    "            Node.player_logits[self.player].append(self.logits)\n",
    "            \n",
    "            if self.depth < Node.MAX_DEPTH:\n",
    "                for action in possible_actions:\n",
    "                    self.children[action] = Node(parent=self, last_action=action)\n",
    "            else:\n",
    "                previous_node = self\n",
    "                for i in range(Node.MAX_DEPTH - 3):\n",
    "                    previous_node = previous_node.parent\n",
    "                would_start_with = previous_node.last_action\n",
    "                if (would_start_with[0] == \"L\") or ((Node.MAX_DEPTH - self.player) % 2):\n",
    "                    for action in possible_actions:\n",
    "                        self.children[action] = Node(parent=self, last_action=action)\n",
    "                else:\n",
    "                    Node.deep_nodes.append(self)\n",
    "\n",
    "        if parent is None: # start connecting the deep nodes\n",
    "            print(\"Connecting deepest nodes\")\n",
    "            for node in Node.deep_nodes:\n",
    "                previous_actions = []\n",
    "                previous_node = node\n",
    "                for i in range(Node.MAX_DEPTH - 2): # minus 2 to match the players\n",
    "                    previous_actions.append(previous_node.last_action)\n",
    "                    previous_node = previous_node.parent\n",
    "                # Now find the Node that is similar to node but one depth less.\n",
    "                next_node = self\n",
    "                for i in range(Node.MAX_DEPTH - 2):\n",
    "                    next_node = next_node.children[previous_actions.pop()]\n",
    "                node.children = next_node.children\n",
    "    \n",
    "    def propagate_probability(self, probability):\n",
    "        # the probability is a matrix with rows possible private info of self.player and\n",
    "        # columns possible private info of the opponent. Each entry corresponds to the\n",
    "        # conditional probability of arriving at that node given the private information.\n",
    "        softmaxed = F.softmax(self.logits, dim=-1)\n",
    "        for i, child in enumerate(self.children.values()): # correct order verified.\n",
    "            give_prob = softmaxed[:, i].unsqueeze(1)\n",
    "            new_probability = (give_prob * probability).t()\n",
    "            child.probability += new_probability\n",
    "            if not child.is_leaf:\n",
    "                child.propagate_probability(new_probability)\n",
    "    \n",
    "    def reset_probability(self):\n",
    "        if self.parent is not None:\n",
    "            self.probability = torch.zeros([len(Die.faces) ** n for n in N_DICE])\n",
    "            if self.player:\n",
    "                self.probability = self.probability.t()\n",
    "        if not self.is_leaf:\n",
    "            for child in self.children.values():\n",
    "                if child.depth > self.depth:\n",
    "                    child.reset_probability()\n",
    "\n",
    "    def who_wins(self, my_roll, opponent_roll):\n",
    "        assert self.is_leaf\n",
    "        all_dice = my_roll + opponent_roll\n",
    "        claim = self.parent.last_action # This is my claim\n",
    "        response = self.last_action # call C or doubt D. My opponent said this.\n",
    "        face = claim[0]\n",
    "        quantity = len(claim)\n",
    "        true_count = all_dice.count(face)\n",
    "        if face != \"L\":\n",
    "            true_count += all_dice.count(\"L\")\n",
    "        if response == \"C\":\n",
    "            if quantity == true_count:\n",
    "                return 1 - self.player\n",
    "            return self.player\n",
    "        if response == \"D\":\n",
    "            if true_count >= quantity:\n",
    "                return self.player\n",
    "            return 1 - self.player\n",
    "\n",
    "    def compute_winner_matrix(self):\n",
    "        assert self.is_leaf\n",
    "        self.winner = torch.zeros([len(Die.faces) ** n for n in N_DICE]) # zeros or ones does not matter\n",
    "        if self.player == 1:\n",
    "            self.winner = self.winner.t()\n",
    "        shape = self.winner.shape\n",
    "        for i in range(shape[0]): # my roll index\n",
    "            for j in range(shape[1]): # opponent roll index\n",
    "                my_roll = Node.index2roll(i, N_DICE[self.player])\n",
    "                opponent_roll = Node.index2roll(j, N_DICE[1 - self.player])\n",
    "                winner = self.who_wins(my_roll, opponent_roll)\n",
    "                self.winner[i, j] = winner\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6f91b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "players = [Player(i, human=i) for i in range(len(N_DICE))]\n",
    "file_name = \"grandfather.pkl\"\n",
    "if os.path.exists(file_name):\n",
    "    # Open the file in binary read mode\n",
    "    with open(file_name, 'rb') as file:\n",
    "        # Load the pickle object\n",
    "        grandfather_node = pickle.load(file)\n",
    "    print(\"Pickle object loaded successfully.\")\n",
    "else:\n",
    "    print(\"Could not find \" + file_name)\n",
    "    grandfather_node = Node(None)\n",
    "Player.faceoff(players[0], players[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285f9f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_1_wins(leaf):\n",
    "    prob_1_wins = leaf.probability * leaf.winner\n",
    "    if leaf.player:\n",
    "        prob_1_wins = prob_1_wins.t()\n",
    "    return prob_1_wins\n",
    "\n",
    "# def get_advantage(leaf, prob_1_wins=0.5):\n",
    "#     score_1 = torch.sum(leaf.probability * leaf.winner)\n",
    "#     score_0 = torch.sum(leaf.probability) - score_1\n",
    "#     return(score_1 / prob_1_wins - score_0 / (1 - prob_1_wins))\n",
    "\n",
    "def get_entropy(leaf):\n",
    "    entropy = - leaf.probability * torch.log(leaf.probability) / np.log(2)\n",
    "    if leaf.player:\n",
    "        entropy = entropy.t()\n",
    "    return entropy\n",
    "\n",
    "def get_loss(player, balance_factor=0.01):\n",
    "    stacked = torch.stack([get_prob_1_wins(leaf) for leaf in Node.leaves], dim=0)\n",
    "    prob_1_wins = torch.sum(stacked) / (len(Die.faces) ** np.sum(N_DICE))\n",
    "    loss = (-1) ** player * prob_1_wins\n",
    "\n",
    "    # stacked = torch.Tensor([get_advantage(leaf, prob_1_wins) ** 2 for leaf in Node.leaves])\n",
    "    stacked = torch.stack([- get_entropy(leaf) for leaf in Node.leaves], dim=0)\n",
    "    loss += balance_factor * torch.sum(stacked)\n",
    "    return loss\n",
    "\n",
    "def calculate_equilibrium(lr = 0.005):\n",
    "    opt_p0 = torch.optim.Adam(Node.player_logits[0], lr=lr)\n",
    "    opt_p1 = torch.optim.Adam(Node.player_logits[1], lr=lr)\n",
    "\n",
    "    for it in range(1500, 2000):\n",
    "        balance_factor = 1 / (it + 1)\n",
    "\n",
    "        opt_p0.zero_grad()\n",
    "        grandfather_node.reset_probability()\n",
    "        grandfather_node.propagate_probability(grandfather_node.probability)\n",
    "        loss = get_loss(0, balance_factor)\n",
    "        loss.backward()\n",
    "        opt_p0.step()\n",
    "        \n",
    "        if it % 10 == 0:\n",
    "            print(f\"iter {it:4d}   loss_0 ≈ {loss.item(): .4f}\")\n",
    "\n",
    "        opt_p1.zero_grad()\n",
    "        grandfather_node.reset_probability()\n",
    "        grandfather_node.propagate_probability(grandfather_node.probability)\n",
    "        loss = get_loss(1, balance_factor)\n",
    "        loss.backward()\n",
    "        opt_p1.step()\n",
    "\n",
    "        if it % 10 == 0:\n",
    "            print(f\"iter {it:4d}   loss_1 ≈ {loss.item(): .4f}\")\n",
    "\n",
    "calculate_equilibrium()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988dcb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the object deeply\n",
    "with open('grandfather.pkl', 'wb') as f:\n",
    "    pickle.dump(grandfather_node, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed873bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "grandfather_node.reset_probability()\n",
    "grandfather_node.propagate_probability(grandfather_node.probability)\n",
    "stacked = torch.stack([get_prob_1_wins(leaf) for leaf in Node.leaves], dim=0)\n",
    "prob_1_wins = torch.sum(stacked, dim=(0, 2))\n",
    "print(prob_1_wins) # according to P0 from grandfather_node\n",
    "\n",
    "grandfather_node.reset_probability()\n",
    "grandfather_node.children[\"4\"].propagate_probability(grandfather_node.probability)\n",
    "stacked = torch.stack([get_prob_1_wins(leaf) for leaf in Node.leaves], dim=0)\n",
    "prob_1_wins = torch.sum(stacked, dim=(0, 2)) \n",
    "print(prob_1_wins) # according to P0 from grandfather_node.children[\"4\"]\n",
    "\n",
    "# In this case, when P0 has rolled [\"L\"], they should increase the probability of\n",
    "# starting with \"4\". That is, if the probability o 1 winning decreases after making\n",
    "# the choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e23da996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.0354e-01, 6.4222e-01, 5.9244e-05, 6.6194e-05, 6.6172e-05, 5.3805e-02,\n",
      "         5.3831e-05, 1.0335e-04, 8.4903e-05],\n",
      "        [3.3706e-01, 4.1194e-01, 1.3712e-01, 5.2765e-05, 5.8712e-05, 1.4944e-03,\n",
      "         6.8451e-05, 1.2460e-04, 1.1209e-01],\n",
      "        [1.9957e-01, 2.6847e-01, 4.8446e-05, 1.4014e-01, 5.9722e-05, 1.2140e-02,\n",
      "         6.9101e-05, 1.2413e-04, 3.7937e-01],\n",
      "        [4.2919e-01, 2.8747e-01, 4.9821e-05, 5.6787e-05, 9.2614e-02, 3.5414e-03,\n",
      "         7.2458e-05, 1.3336e-04, 1.8686e-01],\n",
      "        [9.5311e-05, 3.1804e-04, 2.2172e-06, 2.5483e-06, 2.9214e-06, 9.9956e-01,\n",
      "         3.4969e-06, 6.9924e-06, 6.9924e-06],\n",
      "        [9.9975e-01, 1.7504e-04, 5.0384e-06, 5.4804e-06, 5.8154e-06, 7.3927e-06,\n",
      "         5.3278e-06, 1.1104e-05, 3.1350e-05]], grad_fn=<SoftmaxBackward0>)\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'Node' has no attribute 'player_logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(Node.leaves))\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(Node.n_nodes - \u001b[38;5;28mlen\u001b[39m(Node.leaves))\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28mprint\u001b[39m([\u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mNode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplayer_logits\u001b[49m])\n\u001b[32m      7\u001b[39m test_leaf = grandfather_node.children[\u001b[33m\"\u001b[39m\u001b[33m2\u001b[39m\u001b[33m\"\u001b[39m].children[\u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m].children[\u001b[33m\"\u001b[39m\u001b[33mD\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(test_leaf.winner)\n",
      "\u001b[31mAttributeError\u001b[39m: type object 'Node' has no attribute 'player_logits'"
     ]
    }
   ],
   "source": [
    "print(F.softmax(grandfather_node.children[\"3\"].children[\"5\"].logits, dim=-1))\n",
    "print(Node.n_nodes)\n",
    "print(len(Node.leaves))\n",
    "print(Node.n_nodes - len(Node.leaves))\n",
    "print([len(x) for x in Node.player_logits])\n",
    "\n",
    "test_leaf = grandfather_node.children[\"2\"].children[\"L\"].children[\"D\"]\n",
    "print(test_leaf.winner)\n",
    "grandfather_node.reset_probability()\n",
    "grandfather_node.propagate_probability(grandfather_node.probability)\n",
    "print(torch.sum(test_leaf.probability * test_leaf.winner) / torch.sum(test_leaf.probability))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c4491",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Node.index2roll(Node.roll2index([\"L\", \"5\", \"4\"]), 3))\n",
    "print(Node.MAX_DEPTH)\n",
    "print(grandfather_node.children[\"66\"])\n",
    "print(grandfather_node.children[\"55\"].children[\"66\"])\n",
    "print(grandfather_node.children[\"44\"].children[\"55\"].children[\"66\"])\n",
    "print(grandfather_node.children[\"33\"].children[\"44\"].children[\"55\"].children[\"66\"])\n",
    "print(grandfather_node.children[\"22\"].children[\"33\"].children[\"44\"].children[\"55\"].children[\"66\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "244ec505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0725, -0.0725, -0.0725, -0.0725, -0.0725],\n",
      "        [-0.5391, -0.5391, -0.5391, -0.5391, -0.5391],\n",
      "        [ 0.4722,  0.4722,  0.4722,  0.4722,  0.4722],\n",
      "        [-0.6260, -0.6260, -0.6260, -0.6260, -0.6260]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(4, 1)\n",
    "b = torch.randn(4, 5)\n",
    "a = a.expand((4,5))\n",
    "print(a)\n",
    "Die.faces.index(\"3\")\n",
    "\n",
    "a = (1,2)\n",
    "torch.var(torch.Tensor([2,2,2]))\n",
    "str([\"A\", \"B\"])\n",
    "np.random.choice(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905047ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "grandfather_node.probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fd2094",
   "metadata": {},
   "source": [
    "### Insights from Von Neuman's work and ChatGPT\n",
    "\n",
    "- At each step, for each player, alternating between them, I should optimize for the probability distribution that maximizes the active player's overall expected probability of winning.\n",
    "- This can be done by computing the expected winning probability of the *entire tree*, and maximizing the probability of the winning leaves by only changing the player's own nodes.\n",
    "\n",
    "Something like this:\n",
    "```python\n",
    "# Optimizers for max (P1) and min (P2)\n",
    "opt_p1 = torch.optim.Adam([p1_logits], lr=1e-1)\n",
    "opt_p2 = torch.optim.Adam([p2_logits], lr=1e-1)\n",
    "\n",
    "for it in range(5000):\n",
    "    # — P1 update (ascent) —\n",
    "    opt_p1.zero_grad()\n",
    "    ev = expected_payoff()\n",
    "    (-ev).backward(retain_graph=True)   # gradient of –E wrt p1_logits\n",
    "    opt_p1.step()\n",
    "\n",
    "    # — P2 update (descent) —\n",
    "    opt_p2.zero_grad()\n",
    "    ev = expected_payoff()\n",
    "    (ev).backward()                     # gradient of  E wrt p2_logits\n",
    "    opt_p2.step()\n",
    "\n",
    "    if it % 500 == 0:\n",
    "        print(f\"iter {it:4d}   EV ≈ {ev.item(): .4f}\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
